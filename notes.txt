***** CHAPTER 1 *****


The course "CM20315 - Machine Learning: Introduction to Deep Learning" focuses on building intelligent agents that can improve performance through experience, touching on topics like AI, machine learning, and deep learning. It introduces key tools such as Google Colab, PyTorch, TensorFlow, and Keras for deep learning development.

Supervised Learning
Supervised learning is described as mapping input to output using paired data examples. Different types of supervised learning include:

Regression (continuous output): Tasks like univariate and multivariate regression.
Classification (discrete output): Models such as transformers for text classification and convolutional networks for image classification.
Deep Neural Networks: Defined as flexible families of equations used in deep learning, these models are trained to fit the input-output mappings.
Unsupervised Learning
Unsupervised learning, where the system learns from unlabeled data, includes clustering, finding outliers, and generating new data. The document highlights models like generative adversarial networks (GANs) and variational autoencoders (VAEs) that can generate new examples or fill in missing data.

Reinforcement Learning
Reinforcement learning involves learning by interacting with the environment, focusing on actions, states, and rewards. An example provided is chess, where the goal is to take actions (valid moves) to achieve rewards (taking pieces). Challenges include stochasticity, temporal credit assignment, and exploration-exploitation trade-offs.

Landmarks in Deep Learning
Key milestones in deep learning include:

1958: The perceptron, a simple neural model.
1986: Backpropagation, enabling practical deep neural networks.
2012: AlexNet, a breakthrough in image classification using supervised learning.
2016: AlphaGo, an advanced system using reinforcement learning.
2022: DALL-E2 and ChatGPT, significant developments in image synthesis and natural language generation.
Course Focus
The course emphasizes understanding and improving deep neural networks, with particular attention to specialized networks for images (e.g., image classification and segmentation), text (e.g., text generation and translation), and generative models (e.g., unsupervised learning for generating new data).















***** CHAPTER 2 *****

Introduction to Supervised Learning
Supervised learning is a machine learning approach where the model learns to map inputs to outputs based on a labeled dataset. This process involves using input-output pairs to train the model, meaning the desired output for each input is known during the training phase.

The model in supervised learning is essentially a mathematical equation that relates the input (features) to the output (predicted values). The process of computing outputs from inputs is called inference. The model includes parameters, which influence the output, and the goal of training is to find the best parameters that minimize errors in prediction.

Key Components of Supervised Learning
Model:
In supervised learning, the model is represented by a family of mathematical equations. These equations define how the inputs are transformed into outputs. The model can vary in complexity, from simple linear models to more sophisticated neural networks.

Loss Function:
The loss function measures how poorly the model's predictions align with the actual outputs in the training data. It quantifies the error between the predicted values and the actual values. A common loss function is the least squares loss, which is used in regression tasks. The goal of the model during training is to minimize this loss function, which is achieved by adjusting the parameters to reduce prediction errors.

Training Process:
Training the model involves finding the best parameters (e.g., the weights and biases in a neural network) that minimize the loss function. The technique used for this purpose is often gradient descent, which iteratively adjusts the parameters in the direction that reduces the error.

Testing:
Once a model is trained, it is evaluated on a separate dataset called the test set. This dataset was not used during training, and it helps in assessing how well the model generalizes to new, unseen data. The model's performance on this dataset is crucial because it indicates whether the model can make accurate predictions beyond the training data.

Example: 1D Linear Regression
A concrete example used to explain supervised learning is the 1D linear regression model. In this case, the input is a single feature (e.g., the age of a car), and the output is a real value (e.g., the price of a car). The model equation takes the form:

ùë¶
=
ùëö
ùë•
+
ùëê
y=mx+c

Where:

ùë¶
y is the predicted output (price of the car),
ùëö
m is the slope (a parameter),
ùëê
c is the y-intercept (another parameter),
ùë•
x is the input (e.g., the age of the car).
The goal of training the model is to find the best values for 
ùëö
m and 
ùëê
c that minimize the prediction error. The least squares loss function is used here, which calculates the sum of the squared differences between the predicted and actual values. Minimizing this loss function ensures that the model fits the training data as well as possible.

Training Process in Depth
The document discusses the use of gradient descent to minimize the loss function. Gradient descent is an optimization technique that iteratively adjusts the model‚Äôs parameters by calculating the gradient (or slope) of the loss function and moving in the direction that reduces the error.

The model starts with some initial parameters.
For each iteration, the gradient of the loss function with respect to the parameters is calculated.
The parameters are updated by moving in the opposite direction of the gradient, thereby reducing the loss.
This process continues until the loss reaches a minimum or a stopping criterion is met.
Interactive examples are provided in the document (available through external links) that allow users to visualize how the model's predictions change as the parameters are adjusted and how the loss function behaves during training.

Overfitting and Generalization
A critical issue in supervised learning is overfitting. Overfitting occurs when the model becomes too complex and captures not just the underlying pattern in the data but also the noise or peculiarities of the training set. As a result, the model may perform well on the training data but poorly on unseen data. This lack of generalization means the model fails to make accurate predictions on new data.

The document emphasizes the need to strike a balance between model complexity and generalization:

A model that is too simple might underfit the data, meaning it doesn‚Äôt capture the important patterns.
A model that is too complex might overfit, capturing noise as if it were part of the pattern.
By evaluating the model on a test dataset, one can measure its ability to generalize. If the performance on the test set is significantly worse than on the training set, overfitting is likely.

Where Supervised Learning is Heading
The document also touches on more advanced topics that extend beyond basic linear models:

Shallow Neural Networks: These are more flexible models that can capture more complex relationships between inputs and outputs than linear models.
Deep Neural Networks: These models consist of multiple layers and are even more flexible, capable of learning very complex patterns in data.
Loss Functions: The document hints at exploring other types of loss functions besides least squares, which may be better suited for different kinds of learning tasks.
Training Neural Networks: Methods like gradient descent and its variants are introduced as essential for training these more complex models.
Measuring Performance: Generalization, or the ability of a model to perform well on unseen data, remains the key metric for evaluating the success of supervised learning models.
Conclusion
This chapter provides a comprehensive introduction to supervised learning, from basic concepts to the nuances of training, testing, and evaluating models. By walking through a simple example like 1D linear regression, the document builds a foundation for more advanced topics such as neural networks, deep learning, and the optimization techniques necessary to train these models. Understanding the balance between model complexity and generalization is crucial for building effective supervised learning systems.























***** CHAPTER 3 *****

Shallow Neural Networks Overview
Shallow neural networks are flexible models that can describe arbitrarily complex input-output mappings. Unlike linear regression models, which are limited to representing straight-line relationships, shallow neural networks can capture more complex, non-linear relationships. The networks can take as many inputs and outputs as needed and adjust their internal parameters to fit data better.

Key Concepts in Shallow Neural Networks
Activation Function: A key component in neural networks, transforming input into output. One example is the Rectified Linear Unit (ReLU), which helps the network model complex functions by introducing non-linearity.
Hidden Units: The nodes in the network‚Äôs hidden layer, where intermediate computations occur before the final output is produced. In a shallow network, these hidden units create piecewise linear functions.
Universal Approximation Theorem: This theorem states that, given enough hidden units, a shallow neural network can approximate any continuous function to arbitrary precision. This makes shallow neural networks highly versatile.
Inference: The process of computing outputs from inputs based on the network‚Äôs parameters. After training, a neural network uses the learned parameters to make predictions on new data.
Example of Shallow Neural Networks
An example network might consist of:

1 input and 1 output, with several hidden units that compute intermediate values.
Each hidden unit applies a linear function to its input, followed by an activation function like ReLU, and the results are summed to produce the final output.
This setup can be extended to handle multiple inputs and outputs, forming a network that can model complex functions in higher dimensions.

Training the Network
Training a shallow neural network involves finding the best parameters (weights and biases) that minimize the loss function (e.g., least squares). This process is typically done through gradient descent, where the parameters are adjusted iteratively to reduce the error between the network's predictions and the actual outputs.

Model Capacity and Terminology
Capacity: The number of hidden units in the network determines its capacity to model complex relationships. More hidden units mean greater capacity.
Nomenclature:
Weights: The parameters that determine the slope of the function computed by the network.
Biases: Offsets added to the function to allow for more flexibility in fitting data.
Fully Connected Network: Every unit in one layer is connected to every unit in the next layer, allowing for dense interactions between inputs and outputs.
Feedforward Network: A network where information flows in one direction, from input to output, without loops.
Shallow Neural Network: A network with only one hidden layer.
Deep Neural Network: A network with more than one hidden layer.
Example Networks with Multiple Inputs and Outputs
A shallow network with multiple inputs and outputs can represent more complex functions. For example, a network with three inputs, three hidden units, and two outputs can model relationships in higher-dimensional spaces.
Next Steps in Learning
The document hints at the next level of complexity, where one neural network is fed into another, forming deep neural networks, which can model even more intricate patterns in data.

In summary, shallow neural networks provide a significant step up from simpler models by introducing flexibility and the ability to model complex, non-linear relationships. They are a key part of the toolkit for solving more sophisticated machine learning tasks and serve as the foundation for deeper architectures.





















***** CHAPTER 4 *****

Key Concepts of Deep Neural Networks
Deep Networks Structure Deep neural networks are characterized by having more than one hidden layer, which allows them to capture complex patterns in data. In contrast to shallow networks, the intuition behind how these networks work becomes less straightforward as more layers are added.

Composing Networks Deep networks are essentially a composition of multiple shallow networks. This composition allows for a more powerful representation of the input-output relationships. The document explains how two networks can be combined, and this forms the basis of deep learning architectures.

Hyperparameters The depth (number of layers) and width (number of hidden units per layer) of the network are called hyperparameters. These values are chosen before training begins, and optimizing them is crucial to achieving good performance. Hyperparameter optimization or search involves retraining the model with different configurations to find the best combination.

Shallow vs. Deep Networks The document compares shallow and deep networks, highlighting that while both follow the universal approximation theorem (meaning they can approximate any function with enough hidden units), deep networks have distinct advantages:

Linear Regions per Parameter: Deep networks can create many more linear regions per parameter than shallow networks. This allows deep networks to better capture complex, high-dimensional data patterns.
Depth Efficiency: Some functions that can be efficiently modeled by deep networks would require exponentially more hidden units in shallow networks.
Structured Networks: In tasks like image processing, where inputs can have millions of features (e.g., pixels), fully connected shallow networks are impractical. Deep networks, especially convolutional networks, can share weights across local regions, enabling more efficient processing.
Fitting and Generalization: Deep networks often generalize better and fit the data more effectively than shallow networks, although training deeper networks can become more challenging beyond a certain depth (around 20 layers).
Practical Example of Deep Neural Networks
An example of a two-layer network is given, where one network's hidden units are fed into the next. This allows for increasingly complex representations of data.

Efficiency and Performance of Deep Networks
Deep networks are more efficient in terms of representing certain functions compared to shallow networks. Some real-world functions may require fewer hidden units in deep networks to achieve the same level of approximation.
Deep networks are particularly effective in fields such as computer vision, natural language processing, generative models, and reinforcement learning. They are used in applications like image recognition, text generation, and game playing (e.g., AlphaGo).
Fitting and Generalization
Deep models tend to fit data better than shallow models. However, as the number of layers increases, training deep networks can become harder without using specific techniques to improve training. Generalization, which refers to how well the model performs on unseen data, also improves with deep networks, though the reasons for this are not fully understood.

Hyperparameters in Deep Networks
Depth: Number of layers in the network.
Width: Number of hidden units in each layer. Choosing the right combination of depth and width is crucial, and the process of finding the optimal configuration is referred to as hyperparameter tuning.
Future Directions
The document suggests the next steps in understanding deep networks:

Training deep networks: How to choose appropriate loss functions and optimization techniques to minimize the loss function.
Testing: Evaluating the performance of deep networks on unseen data to ensure they generalize well.
In summary, deep neural networks, by stacking multiple layers of hidden units, are capable of modeling much more complex functions than shallow networks. They have been particularly successful in applications that involve large datasets and intricate patterns, such as image and text recognition. Despite the challenges in training them, deep networks have become the standard for many cutting-edge machine learning applications due to their superior ability to generalize and fit complex data.























***** CHAPTER 6 *****

Key Concepts:
1. Math Overview
2. Gradient Descent Algorithm
3. Linear Regression Example
4. Gabor Model Example
5. Stochastic Gradient Descent (SGD)
6. Momentum
7. Adam Optimizer

Gradient Descent Algorithm:
- Step 1: Compute derivatives (slopes) with respect to parameters.
- Step 2: Update parameters according to the rule: Œ± = step size (learning rate).

Convex and Non-Convex Problems:
- Test for convexity: The second derivative is positive everywhere.
- In higher dimensions: The determinant of the Hessian matrix (2nd derivative matrix) is positive.

Gabor Model:
- Gradient descent finds the global minimum if the starting point is in the right ‚Äúvalley.‚Äù
- Otherwise, it may descend to a local minimum or get stuck near a saddle point.

Stochastic Gradient Descent (SGD):
- Computes the gradient based on a subset of data points (mini-batch).
- Uses the dataset without replacement, one pass through the data is called an epoch.
- Benefits: Escapes local minima, uses all data equally, is computationally efficient.
- Disadvantages: Doesn't converge traditionally, needs a learning rate schedule.

Momentum:
- Weighted sum of the current gradient and the previous gradient.
- Nesterov accelerated momentum: Predict the next step, then measure the gradient.

Adam Optimizer (Adaptive Moment Estimation):
- Combines the benefits of momentum with adaptive learning rates.
- Measures mean and pointwise squared gradients.
- Adjusts parameters accordingly.

Key Hyperparameters:
- Choice of learning algorithm
- Learning rate
- Momentum











***** CHAPTER 7 *****

Key Concepts:
1. Multiclass classification problem (e.g., music genre classification).
2. Loss function: Measures how well a model maps inputs to outputs (smaller is better).
3. Problem 1: Computing gradients efficiently for neural networks.
   - The backpropagation algorithm is used for this purpose.
   - Derivatives need to be computed for every parameter, point in the batch, and iteration of SGD.
4. Problem 2: Initialization of parameters before running SGD.

Backpropagation:
1. Forward Pass:
   - Activations are multiplied by weights in the previous layer (e.g., ReLU output).
   - We need to know how activations affect the loss.
2. Backward Pass:
   - Calculate how a small change in a weight or bias modifies the loss.
   - Chain rule is applied to compute these gradients.

Toy Model:
- A simplified function using scalars (e.g., sin, exp, cos).
- Forward and backward passes are used to calculate gradients for composed functions.
- Derivatives of these intermediate quantities are computed in reverse order using the chain rule.

Matrix Calculus:
- Scalar function of vector and matrix derivatives.
- Comparison of scalar and matrix derivatives, as well as their calculation methods.

Algorithmic Differentiation:
- Modern deep learning frameworks like PyTorch compute derivatives automatically using algorithmic differentiation.
- Each component (e.g., ReLU, linear functions) knows how to compute its own derivative.
- The framework computes the chain of derivatives through the network.

PyTorch Example:
- Define a neural network and initialize parameters using He initialization.
- Choose a loss function and optimization algorithm (e.g., Adam).
- Set learning rate and learning rate schedule.
- Train the model for 100 batches using random data.

Backpropagation Pros and Cons:
- Pros: Extremely efficient, requires only matrix multiplication and ReLU functions.
- Cons: Memory-intensive, sequential processing, handling multiple batches in parallel can be challenging.

Summary:
- Backpropagation is a powerful and efficient algorithm but can be computationally expensive in terms of memory and resources.
- Algorithmic differentiation enables automatic gradient computation, simplifying the training process in deep learning frameworks.


***** CHAPTER 8 *****